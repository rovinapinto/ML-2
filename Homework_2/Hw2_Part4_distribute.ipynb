{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaA1H2AHyVWQ"
   },
   "source": [
    "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
    "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
    "# Part 4 : Modelling with embeddings!\n",
    "**ML-2 Cohort 1** <br>\n",
    "**Instructor: Dr. Rahul Dave**<br>\n",
    "**Max Score: 100** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6jBFtovs3nh1"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Activation, Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "import pickle\n",
    "import csv\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlux5Gn-n-Gz"
   },
   "source": [
    "We will repeat the first initial steps again from Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5Hi8ehR7WFX"
   },
   "source": [
    "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "arl_aj6X7VQk"
   },
   "outputs": [],
   "source": [
    "f = open('feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cwMxtFTUDRFN"
   },
   "outputs": [],
   "source": [
    "# we will split the train_ratio and 90% and 10% and set the train_size\n",
    "train_ratio = 0.9\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XpP1Emg2Dni0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0, 1058,    1,    0,    0,    0,    0,    1]), 4491)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at our data\n",
    "X[1], y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2fhYw49Dz3w"
   },
   "source": [
    "The next set of inputs is following: Write the code for this yourself\n",
    "\n",
    "1. Do you want to one hot encode the data?\n",
    "2. Do you want to provide embeddings as input - this will be set to True for models with entity embeddings\n",
    "3. Do you want to save the emmbeddings? - again set to true if you want to entity embeddings\n",
    "4. if 3 is set to true, we want to save them to a embeddings.pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Tqtv6VWWDuZu"
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "embeddings_as_input=True\n",
    "one_hot_as_input = False\n",
    "save_embeddings = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ayLSo_1E0yF"
   },
   "source": [
    "## Now lets work with Models with Entity embedding!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTYK-isysXiu"
   },
   "source": [
    "Now that you have saved embeddings - push this into the other models as an input with X. \n",
    "\n",
    "How will we do this? \n",
    "\n",
    "We need to update our X values. \n",
    "\n",
    "1. We will define a function called embed_features, which will combine the embedding with X. \n",
    "2. Call this function and update it with the inital X values taken from the pickle file - features_train_data\n",
    "3. Then split you data, X_emb - into Xtrain and X_Val, y_train and y_val remain the same\n",
    "4. Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DbymoNn65ETi"
   },
   "outputs": [],
   "source": [
    "#call our saved embeddings from part 3\n",
    "saved_embeddings_fname = 'files/embeddings.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wUVh51hgqCyp"
   },
   "outputs": [],
   "source": [
    "#combining embedding\n",
    "def embed_features(X, saved_embeddings_fname):\n",
    "    f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
    "    embeddings = pickle.load(f_embeddings)\n",
    "\n",
    "    index_embedding_mapping = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4, 7: 5}\n",
    "    X_embedded = []\n",
    "\n",
    "    (num_records, num_features) = X.shape\n",
    "    for record in X:\n",
    "        embedded_features = []\n",
    "        for i, feat in enumerate(record):\n",
    "            feat = int(feat)\n",
    "            if i not in index_embedding_mapping.keys():\n",
    "                embedded_features += [feat]\n",
    "            else:\n",
    "                embedding_index = index_embedding_mapping[i]\n",
    "                embedded_features += embeddings[embedding_index][feat].tolist()\n",
    "\n",
    "        X_embedded.append(embedded_features)\n",
    "\n",
    "    return np.array(X_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf8U7znjHZrW"
   },
   "source": [
    "**Explain what is happening the function above**\n",
    "\n",
    "The above function will replace all the features except `'store_is_open'` and `'promo'` to their new value which are saved in `embeddings.pickle` in part3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALCfACl0rzeK"
   },
   "source": [
    "### Embedding with X - input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-wcLbyHps8_W"
   },
   "outputs": [],
   "source": [
    "#check if embedding is needed, if yes call embed_features - with X and the embeddings passed to it - call this new X as X_emb\n",
    "if embeddings_as_input:\n",
    "    X_emb = embed_features(X, saved_embeddings_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDzhYvCJHm8d"
   },
   "source": [
    "Split the train and validation based on train size and on the new X_emb values from the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hLTrTSGPEaGw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((759904, 42), (759904, 42))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#update the X_train and X_val\n",
    "X_train, X_val = X_emb[:train_size], X_emb[:train_size]\n",
    "y_train, y_val = y[:train_size], y[:train_size]\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "StnmulRkEnRt"
   },
   "outputs": [],
   "source": [
    "def sample(X, y, n):\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = np.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ednNv14aEpjS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1I6AT5MuRTH"
   },
   "source": [
    "## Add the embeddings into the Models and check their MAPE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmJYNA0PH5AU"
   },
   "source": [
    "All the models defined here will have the same parameters as the ones defined in Part 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QTRegclpugs3"
   },
   "outputs": [],
   "source": [
    "#defining mape\n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    y_true, y_pred = np.array(Y_actual), np.array(Y_Predicted)\n",
    "    mape = (1/len(y_true)) * np.sum(np.absolute((y_true - y_pred) / y_true))\n",
    "    return mape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4Sjp7NQu1ow"
   },
   "source": [
    "### Model 1: Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kGX-_sRYkxPv"
   },
   "outputs": [],
   "source": [
    "# instantiate a RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1, n_jobs=6)\n",
    "# fit on a training set\n",
    "rf.fit(X_train, np.log(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "pickle.dump(rf, open('models/rf_with_emb.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "rf = joblib.load('models/rf_with_emb.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAPE (with emb): 0.0337\n",
      "Validation MAPE (with emb): 0.0336\n"
     ]
    }
   ],
   "source": [
    "# make predictions on training and validation sets\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_val = rf.predict(X_val) \n",
    "\n",
    "# calculate MAPEs\n",
    "rf_train_mape = MAPE(np.log(y_train), y_pred_train)\n",
    "rf_val_mape = MAPE(np.log(y_val), y_pred_val)\n",
    "\n",
    "print(\"Training MAPE (with emb): {:.4f}\\nValidation MAPE (with emb): {:.4f}\".format(rf_train_mape, rf_val_mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYvVmYMpu_Vi"
   },
   "source": [
    "### Model 2: Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "m1--s_4pGOuB"
   },
   "outputs": [],
   "source": [
    "# define parameters for xgboost regressor\n",
    "params = {'nthread': -1, 'max_depth': 7,\n",
    "          'eta': 0.02,'silent': 1,\n",
    "          'objective': 'reg:linear',\n",
    "          'colsample_bytree': 0.7,\n",
    "          'subsample': 0.7}\n",
    "\n",
    "# create a dmatrix for training data\n",
    "dmatrix = xgb.DMatrix(data=X_train, label=np.log(y_train))\n",
    "\n",
    "# create dmatrix for train and validation data without target feature\n",
    "feature_Xtr = xgb.DMatrix(X_train)\n",
    "feature_Xval = xgb.DMatrix(X_val)\n",
    "\n",
    "# fit the xgb regressor on training data\n",
    "xgb_reg = xgb.train(dtrain=dmatrix, params=params, num_boost_round=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "pickle.dump(xgb_reg, open('models/xgb_with_emb.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "xgb_reg = joblib.load('models/xgb_with_emb.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAPE (with emb): 0.0357\n",
      "Validation MAPE (with emb): 0.0356\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "y_train_pred = xgb_reg.predict(feature_Xtr)\n",
    "y_val_pred = xgb_reg.predict(feature_Xval)\n",
    "\n",
    "# calculate MAPEs\n",
    "xgb_train_mape = MAPE(np.log(y_train), y_train_pred) \n",
    "xgb_val_mape = MAPE(np.log(y_val), y_val_pred)\n",
    "\n",
    "print(\"Training MAPE (with emb): {:.4f}\\nValidation MAPE (with emb): {:.4f}\".format(xgb_train_mape, xgb_val_mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhM6IVJJv0rU"
   },
   "source": [
    "## Final Commments!\n",
    "\n",
    "Apart from how long this homework was, lets make some other final comments\n",
    "\n",
    "**Question 1: Did models with Entity Embeddings perfom better?**\n",
    "\n",
    "*Yes it did.*\n",
    "\n",
    "**Question 2: Now that you have completed this homework, lets answer the main purpose of the homework - How do you think entity embeddings improved the MAPE score. To show this do a similar table like the one did in Paper**\n",
    "\n",
    "*Entity Embedding is a mapping of categorical variables into n dimensional vector space.\n",
    "Its advantage lies in the ability to intrinsically group similar variables.\n",
    "EE improved the MAPE score as it was able to extract intrinsic properties of each of the features such as dow, year , month etc which have an intrinsic ordering to them (eg Sunday being more closer to Saturday than Wednesday), unlike one hot encoding which cannot preserve or detect such underlying properties. The authors of the referred paper put it much better when they write, “entity embeddings force the network to learn the intrinsic properties of each of the feature as well as the sales distribution in the feature space. One-hot encoding,on the other hand, only learns about the sales distribution.”*\n",
    "\n",
    "![alt text](dow_embedding.png \"Title\")\n",
    "\n",
    "\n",
    "**Question 3: Add a table here to show the similar to the one done in paper - to show the different MAPE values with and without embeddings**\n",
    "\n",
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mape = 0.026257\n",
    "xg_boost_mape = 0.042468\n",
    "nn_mape = 0.012263\n",
    "rf_emb_mape = 0.0336\n",
    "xg_boost_emb_mape = 0.0356\n",
    "nn_emb_mape = ''\n",
    "\n",
    "mapes = {'MAPE': {'Random Forest': rf_mape, 'XGBoost': xg_boost_mape, 'Neural Network': nn_mape},\n",
    "         'MAPE (With EE)': {'Random Forest': rf_emb_mape, 'XGBoost': xg_boost_emb_mape, 'Neural Network': nn_emb_mape}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MAPE (With EE)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.0336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.042468</td>\n",
       "      <td>0.0356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>0.0084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           method      MAPE  MAPE (With EE)\n",
       "0   Random Forest  0.026257          0.0336\n",
       "1         XGBoost  0.042468          0.0356\n",
       "2  Neural Network  0.012263          0.0084"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(mapes)\n",
    "table.reset_index(inplace=True)\n",
    "table.rename(columns={'index': 'method'}, inplace=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Hw2_Part4_distribute.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
