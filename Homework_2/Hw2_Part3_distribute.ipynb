{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaA1H2AHyVWQ"
   },
   "source": [
    "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
    "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
    "# Part 3 : Extracting embeddings!\n",
    "**ML-2 Cohort 1** <br>\n",
    "**Instructor: Dr. Rahul Dave**<br>\n",
    "**Max Score: 100** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6jBFtovs3nh1"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Activation, Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "import pickle\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlux5Gn-n-Gz"
   },
   "source": [
    "We will repeat the first initial steps again from Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5Hi8ehR7WFX"
   },
   "source": [
    "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "arl_aj6X7VQk"
   },
   "outputs": [],
   "source": [
    "f = open('files/feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cwMxtFTUDRFN"
   },
   "outputs": [],
   "source": [
    "# we will split the train_ratio and 90% and 10% and set the train_size\n",
    "train_ratio = 0.9\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XpP1Emg2Dni0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0, 1058,    1,    0,    0,    0,    0,    1]), 4491)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at our data\n",
    "X[1], y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2fhYw49Dz3w"
   },
   "source": [
    "The next set of inputs is following: \n",
    "\n",
    "1. Do you want to one hot encode the data?\n",
    "2. Do you want to provide embeddings as input - this will be set to **True** for models with entity embeddings\n",
    "3. Do you want to save the emmbeddings? - again set to **true** if you want to entity embeddings\n",
    "4. if 3 is set to **true**, we want to save them to a embeddings.pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Tqtv6VWWDuZu"
   },
   "outputs": [],
   "source": [
    "one_hot_as_input = False #using entity embedding instead\n",
    "embeddings_as_input = True #embeddings later on needs to set to true for Part 3\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"files/embeddings.pickle\"  # set save_embeddings to True to create this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCk7tyVQEdy6"
   },
   "source": [
    "Split the data based on the train_size - same as part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hLTrTSGPEaGw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759904, 8) (759904,) (84434, 8) (84434,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = X[:train_size], X[train_size:], y[:train_size], y[train_size:]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "StnmulRkEnRt"
   },
   "outputs": [],
   "source": [
    "def sample(X, y, n):\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = np.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ednNv14aEpjS"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "X_val, y_val = sample(X_val, y_val, 200000)  # Simulate data sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ayLSo_1E0yF"
   },
   "source": [
    "## Now lets work with Models with Entity embedding!!\n",
    "\n",
    "To read about Entity Embeddings in more detail and its use cases look at the follwing links:\n",
    "1. [Enhancing categorical features with Entity Embeddings](https://towardsdatascience.com/enhancing-categorical-features-with-entity-embeddings-e6850a5e34ff)\n",
    "2. [Understanding Entity Embeddings and Itâ€™s Application](https://towardsdatascience.com/understanding-entity-embeddings-and-its-application-69e37ae1501d#:~:text=Loosely%20speaking%2C%20entity%20embedding%20is,a%20sentence%2C%20or%20a%20paragraph.)\n",
    "\n",
    "The basic outline of this model will be something like this:\n",
    "\n",
    "\n",
    "![Outline.jpeg](https://drive.google.com/uc?export=view&id=1cm-jylknGEFg9KjehhRy90INkUJqRwrL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uOEqzd8BzK4"
   },
   "source": [
    "We will be creating the following embeddings:\n",
    "\n",
    "1. input_store - with input shape as (1,) \n",
    "  * The input_store will be passed to output_store to create an embedding layer with embedding shape as 1115, 10\n",
    "  * we will reshape this to target_shape=(10,)\n",
    "2. input_dow - with input shape as (1,) \n",
    "  * The input_dow will be passed to output_dow to create an embedding layer with embedding shape as 7, 6\n",
    "  * we will reshape this to target_shape=(6,)\n",
    "3. input_promo - with input shape as (1,) \n",
    "  * output promo will be a dense(1) for input_promo\n",
    "4. input_year - with input shape as (1,) \n",
    "  * The input_year will be passed to output_year to create an embedding layer with embedding shape as 3, 2\n",
    "  * we will reshape this to target_shape=(2,)\n",
    "5. input_month - with input shape as (1,) \n",
    "  * The input_month will be passed to output_month to create an embedding layer with embedding shape as 12, 6\n",
    "  * we will reshape this to target_shape=(6,)\n",
    "6. input_day - with input shape as (1,) \n",
    "  * The input_day will be passed to output_day to create an embedding layer with embedding shape as 31, 10\n",
    "  * we will reshape this to target_shape=(10,)\n",
    "7. input_germanstate - with input shape as (1,) \n",
    "  * The input_germanstate will be passed to output_germanstate to create an embedding layer with embedding shape as 12, 6\n",
    "  * we will reshape this to target_shape=(6,)\n",
    "\n",
    "\n",
    "* Construct an input model with all the inputs \n",
    "* Construct an output embeddings with all the outputs( basically the embeddings) - concatenate this and call it the **output model**\n",
    "\n",
    "Set the output model with the following parameters:\n",
    "\n",
    "1. Dense Layer - 1000 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
    "2. Dense Layer - 500 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
    "3. Final dense layer with 1 neuron, and activation as sigmoid\n",
    "4. Create a KerasModel called modelNN_emb:\n",
    "\n",
    "       modelNN_emb = KerasModel(inputs=input_model, outputs=output_model)\n",
    "4. Compile the model on mean absolute error and optimizer as adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0qGSxshWqRCu"
   },
   "outputs": [],
   "source": [
    "#Define the embedding NN model\n",
    "\n",
    "input_store = Input(shape=(1,))\n",
    "output_store = Embedding(1115,10, name='store_embedding')(input_store)\n",
    "output_store = Reshape(target_shape=(10,))(output_store)\n",
    "\n",
    "input_dow = Input(shape=(1,))\n",
    "output_dow = Embedding(7,6, name='dow_embedding')(input_dow)\n",
    "output_dow = Reshape(target_shape=(6,))(output_dow)\n",
    "\n",
    "input_promo = Input(shape=(1,))\n",
    "output_promo = Dense(1)(input_promo)\n",
    "\n",
    "input_year = Input(shape=(1,))\n",
    "output_year = Embedding(3,2, name='year_embeding')(input_year)\n",
    "output_year = Reshape(target_shape=(2,))(output_year)\n",
    "\n",
    "input_month = Input(shape=(1,))\n",
    "output_month = Embedding(12,6, name= 'month_embedding')(input_month)\n",
    "output_month = Reshape(target_shape=(6,))(output_month)\n",
    "\n",
    "input_day = Input(shape=(1,))\n",
    "output_day = Embedding(31,10, name='day_embedding')(input_day)\n",
    "output_day = Reshape(target_shape=(10,))(output_day)\n",
    "\n",
    "input_germanstate = Input(shape=(1,))\n",
    "output_germanstate = Embedding(12,6, name='germanstate_embedding')(input_germanstate)\n",
    "output_germanstate = Reshape(target_shape=(6,))(output_germanstate)\n",
    "\n",
    "input_model = [input_store, input_dow, input_promo, input_year, input_month, input_day, input_germanstate]\n",
    "embeddings = [output_store, output_dow, output_promo, output_year, output_month, output_day, output_germanstate]\n",
    "output_model = Concatenate()(embeddings)\n",
    "\n",
    "#output model layers\n",
    "output_model = Dense(1000, kernel_initializer='uniform', activation='relu')(output_model)\n",
    "output_model = Dense(500, kernel_initializer='uniform', activation='relu')(output_model)\n",
    "output_model = Dense(1, activation='sigmoid')(output_model)\n",
    "\n",
    "modelNN_emb = KerasModel(inputs=input_model, outputs=output_model)\n",
    "\n",
    "modelNN_emb.compile(loss='mean_absolute_error', optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "store_embedding (Embedding)     (None, 1, 10)        11150       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dow_embedding (Embedding)       (None, 1, 6)         42          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "year_embeding (Embedding)       (None, 1, 2)         6           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "month_embedding (Embedding)     (None, 1, 6)         72          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "day_embedding (Embedding)       (None, 1, 10)        310         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "germanstate_embedding (Embeddin (None, 1, 6)         72          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 10)           0           store_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 6)            0           dow_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 2)            0           year_embeding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 6)            0           month_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 10)           0           day_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 6)            0           germanstate_embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 41)           0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 dense[0][0]                      \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000)         42000       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 500)          500500      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            501         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 554,655\n",
      "Trainable params: 554,655\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelNN_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tqUNSxR8q3-B"
   },
   "outputs": [],
   "source": [
    "#we will rescale our y_train for the model\n",
    "#the reason for this is mentioned in the paper in the same section \n",
    "max_log_y = max(np.max(np.log(y_train)), np.max(np.log(y_val)))\n",
    "fitting_ytr = np.log(y_train) / max_log_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Np80imB4MgYY"
   },
   "outputs": [],
   "source": [
    "#do the same for y_val as well\n",
    "fitting_yval = np.log(y_val) / max_log_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WcPVMLiF-1v"
   },
   "source": [
    "Now that we have built the model, we need to ensure that the input passed to the model is also in the same format - hence we will define a function split the features we are performing embeddings on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o1ZSq3RetpUx"
   },
   "outputs": [],
   "source": [
    "#split the features\n",
    "def split_features(X):\n",
    "    X_list = []\n",
    "\n",
    "    store_index = X[..., [1]]\n",
    "    X_list.append(store_index)\n",
    "\n",
    "    day_of_week = X[..., [2]]\n",
    "    X_list.append(day_of_week)\n",
    "\n",
    "    promo = X[..., [3]]\n",
    "    X_list.append(promo)\n",
    "\n",
    "    year = X[..., [4]]\n",
    "    X_list.append(year)\n",
    "\n",
    "    month = X[..., [5]]\n",
    "    X_list.append(month)\n",
    "\n",
    "    day = X[..., [6]]\n",
    "    X_list.append(day)\n",
    "\n",
    "    State = X[..., [7]]\n",
    "    X_list.append(State)\n",
    "\n",
    "    return X_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gn0ZRL6at7xg"
   },
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "    X_list = split_features(X)\n",
    "    return X_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJKlX4TdGNcn"
   },
   "source": [
    "1. Add a ModelCheckpoint to save the weights \n",
    "\n",
    "2. Fit the model on the `preprocessing(X_train)` and on the fitting_y with 10 epochs and batch_size as 128 \n",
    "  * add in callback here for ModelCheckPoint and also provide validation data here. \n",
    "  * ModelCheckPoint will be used to save the weights for the model, we will use these weights for each embedding layer( you can read more about model check point [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "BCE5qF0lrEmJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0251 - val_loss: 0.0113\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01129, saving model to models/nn_with_emb.hdf5\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 0.0096 - val_loss: 0.0107\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01129 to 0.01072, saving model to models/nn_with_emb.hdf5\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01072 to 0.01000, saving model to models/nn_with_emb.hdf5\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0080 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01000 to 0.00972, saving model to models/nn_with_emb.hdf5\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0077 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00972 to 0.00972, saving model to models/nn_with_emb.hdf5\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0074 - val_loss: 0.0096\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00972 to 0.00955, saving model to models/nn_with_emb.hdf5\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0071 - val_loss: 0.0092\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00955 to 0.00924, saving model to models/nn_with_emb.hdf5\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0070 - val_loss: 0.0093\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00924\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0069 - val_loss: 0.0093\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00924\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0068 - val_loss: 0.0090\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00924 to 0.00905, saving model to models/nn_with_emb.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2848c2f10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model checkpoint callback to save the model weights\n",
    "modelNN_emb.checkpointer = ModelCheckpoint(filepath=\"models/nn_with_emb.hdf5\", verbose=1, save_best_only=True)\n",
    "modelNN_emb.fit(preprocessing(X_train), fitting_ytr, epochs = 10, batch_size = 128, validation_data= (preprocessing(X_val),fitting_yval), callbacks=[modelNN_emb.checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "modelNN_emb.load_weights('models/nn_with_emb.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on both train and val set\n",
    "y_pred_val = modelNN_emb.predict(preprocessing(X_val)).flatten()\n",
    "y_pred_train = modelNN_emb.predict(preprocessing(X_train)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAPE (with EE): 0.0085\n",
      "Validation MAPE (with EE): 0.0110\n"
     ]
    }
   ],
   "source": [
    "#defining mape\n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    y_true, y_pred = np.array(Y_actual), np.array(Y_Predicted)\n",
    "    mape = (1/len(y_true)) * np.sum(np.absolute((y_true - y_pred) / y_true))\n",
    "    return mape \n",
    "\n",
    "# calculate train and val MAPE\n",
    "train_mape = MAPE(fitting_ytr, y_pred_train)\n",
    "val_mape = MAPE(fitting_yval, y_pred_val)\n",
    "\n",
    "print(\"Train MAPE (with EE): {:.4f}\\nValidation MAPE (with EE): {:.4f}\".format(train_mape, val_mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX4q6sj2qBTy"
   },
   "source": [
    "Now we will save the embeddings from the model defined above:\n",
    "check if save_embeddings is set to True first, if yes store the following:\n",
    "  1. store_embedding\n",
    "  2. dow_embedding\n",
    "  3. year_embedding\n",
    "  4. month_embedding\n",
    "  5. day_embedding\n",
    "  6. state_embedding\n",
    "\n",
    "\n",
    "Save this entire embeddings into the pickle file - **saved_embeddings_fname - embeddings.pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "o7QS4kLLqKrH"
   },
   "outputs": [],
   "source": [
    "if save_embeddings:\n",
    "    store_embedding = modelNN_emb.get_layer('store_embedding').get_weights()[0]\n",
    "    dow_embedding = modelNN_emb.get_layer('dow_embedding').get_weights()[0]\n",
    "    year_embedding = modelNN_emb.get_layer('year_embeding').get_weights()[0]\n",
    "    month_embedding = modelNN_emb.get_layer('month_embedding').get_weights()[0]\n",
    "    day_embedding = modelNN_emb.get_layer('day_embedding').get_weights()[0]\n",
    "    german_states_embedding = modelNN_emb.get_layer('germanstate_embedding').get_weights()[0]\n",
    "    with open(saved_embeddings_fname, 'wb') as f:\n",
    "        pickle.dump([store_embedding, dow_embedding, year_embedding,\n",
    "                     month_embedding, day_embedding, german_states_embedding], f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm5mmmJi_BCc"
   },
   "source": [
    "# You are done with Part 3!! \n",
    "Remember to save the embeddings.pickle - this will be used in Part 4!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Hw2_Part3_distribute.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
